{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For \"Gender swap\" we have used CycleGAN. The architecture is comprised of four models, two discriminator models, and two generator models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](pictures/GanDiagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](pictures/GanDiagram2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>discriminator</b> is a deep convolutional neural network that performs image classification. It takes a source image as input and predicts the likelihood of whether the target image is a real or fake image. Two discriminator models are used, one for Domain-A and one for Domain-B.<p>\n",
    "The output of the model depends on the size of the input image but may be one value or a square activation map of values. Each value is a probability for the likelihood that a patch in the input image is real. These values can be averaged to give an overall likelihood or classification score if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "\n",
    "def discriminator(image_shape):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    in_image = Input(shape=image_shape)\n",
    "    \n",
    "    layer = Conv2D(64, (4,4), strides=(2,2),padding='same',kernel_initializer=init)(in_image)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "\n",
    "    layer = Conv2D(128, (4,4), strides=(2,2),padding='same',kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "\n",
    "    layer = Conv2D(256, (4,4), strides=(2,2),padding='same',kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "\n",
    "    layer = Conv2D(512, (4,4), strides=(2,2),padding='same',kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "\n",
    "    layer = Conv2D(512, (4,4), padding='same',kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "\n",
    "    patch_out = Conv2D(1,(4,4), padding='same', kernel_initializer=init)(layer)\n",
    "\n",
    "    model = Model(in_image, patch_out)\n",
    "\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=0.0002,beta_1=0.5),loss_weights=[0.5])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>generator</b> is an encoder-decoder model architecture. The model takes a source image (e.g. male photo) and generates a target image (e.g. female photo). It does this by first downsampling or encoding the input image down to a bottleneck layer, then interpreting the encoding with a number of ResNet layers that use skip connections, followed by a series of layers that upsample or decode the representation to the size of the output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(n_filters, input_layer):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "\n",
    "    layer = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    layer = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "\n",
    "    layer = Concatenate()([layer, input_layer])\n",
    "\n",
    "    return layer\n",
    "\n",
    "def generator(image_shape, n_resnet=9):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    in_image = Input(shape=image_shape)\n",
    "\n",
    "    layer = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    layer = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    layer = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    for _ in range(n_resnet):\n",
    "        layer = resnet_block(256,layer)\n",
    "    \n",
    "    layer = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    layer = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    layer = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    out_image = Activation('tanh')(layer)\n",
    "\n",
    "    model = Model(in_image, out_image)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each generator model is optimized via the combination of four outputs with four loss functions:\n",
    "\n",
    "- Adversarial loss (L2 or mean squared error).\n",
    "- Identity loss (L1 or mean absolute error).\n",
    "- Forward cycle loss (L1 or mean absolute error).\n",
    "- Backward cycle loss (L1 or mean absolute error).\n",
    "\n",
    "We have achieved this by defining a composite model used to train each generator model that is responsible for only updating the weights of that generator model, although it is required to share the weights with the related discriminator model and the other generator model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_model(g_model_1,d_model,g_model_2,image_shape):\n",
    "    g_model_1.trainable = True\n",
    "    d_model.trainable = False\n",
    "    g_model_2.trainable = False\n",
    "\n",
    "    input_gen = Input(shape=image_shape)\n",
    "    gen1_out = g_model_1(input_gen)\n",
    "    output_d = d_model(gen1_out)\n",
    "\n",
    "    input_id = Input(shape=image_shape)\n",
    "    output_id = g_model_1(input_id)\n",
    "\n",
    "    output_f = g_model_2(gen1_out)\n",
    "\n",
    "    gen2_out = g_model_2(input_id)\n",
    "    output_b = g_model_1(gen2_out)\n",
    "\n",
    "    model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n",
    "\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "    model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "from numpy import ones\n",
    "from numpy import zeros\n",
    "from numpy.random import randint\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def load_real_samples(filename):\n",
    "    data = load(filename)\n",
    "    X1, X2 = data['arr_0'], data['arr_1']\n",
    "\n",
    "    X1 = (X1 - 127.5) / 127.5\n",
    "    X2 = (X2 - 127.5) / 127.5\n",
    "    return [X1,X2]\n",
    "\n",
    "def generate_real_samples(dataset,n_samples, patch_shape):\n",
    "    ix = randint(0, dataset.shape[0],n_samples)\n",
    "    X = dataset[ix]\n",
    "    Y = ones((n_samples,patch_shape,patch_shape,1))\n",
    "    return X, Y\n",
    "\n",
    "def generate_fake_samples(g_model, dataset, patch_shape):\n",
    "    X = g_model.predict(dataset)\n",
    "    Y = zeros((len(X),patch_shape,patch_shape,1))\n",
    "    return X, Y\n",
    "\n",
    "# This function will save each generator model to the current directory in H5 format,\n",
    "# including the training iteration number in the filename\n",
    "def save_models(step, g_model_AtoB, g_model_BtoA):\n",
    "    path = '../../models/'\n",
    "    filename1 = 'g_model_AtoB_%06d.h5' % (step+1)\n",
    "    g_model_AtoB.save(path+filename1)\n",
    "    filename2 = 'g_model_BtoA_%06d.h5' % (step+1)\n",
    "    g_model_BtoA.save(path+filename2)\n",
    "\n",
    "# This function uses a given generator model to generate translated versions of\n",
    "# a few randomly selected source photographs and saves the plot to file.\n",
    "def summarize_performance(step, g_model, trainX, name, n_samples=5):\n",
    "    path = '../../models/'\n",
    "    X_in, _ = generate_real_samples(trainX, n_samples, 0)\n",
    "    X_out, _ = generate_fake_samples(g_model, X_in, 0)\n",
    "    X_in = (X_in + 1) / 2.0\n",
    "    X_out = (X_out + 1) / 2.0\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(2, n_samples, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_in[i])\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(2, n_samples, 1 + n_samples + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_out[i])\n",
    "    filename1 = '%s_generated_plot_%06d.png' % (name, (step+1))\n",
    "    pyplot.savefig(path+filename1)\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_image_pool(pool, images, max_size=50):\n",
    "    selected = list()\n",
    "    for image in images:\n",
    "        if len(pool) < max_size:\n",
    "            pool.append(image)\n",
    "            selected.append(image)\n",
    "        elif random() < 0.5:\n",
    "            selected.append(image)\n",
    "        else:\n",
    "            ix = randint(0, len(pool))\n",
    "            selected.append(pool[ix])\n",
    "            pool[ix] = image\n",
    "    return asarray(selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train() function takes all six models (two discriminator, two generator, and two composite models) as arguments along with the dataset and trains the models.\n",
    "\n",
    "The order of model updates is implemented to match the official Torch implementation. First, a batch of real images from each domain is selected, then a batch of fake images for each domain is generated. The fake images are then used to update each discriminatorâ€™s fake image pool.\n",
    "\n",
    "Next, the Generator-A model (male to female) is updated via the composite model, followed by the Discriminator-A model (female). Then the Generator-B (female to male) composite model and Discriminator-B (male) models are updated.\n",
    "\n",
    "Loss for each of the updated models is then reported at the end of the training iteration. Importantly, only the weighted average loss used to update each generator is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset):\n",
    "    n_epochs, n_batch, = 100, 1\n",
    "    n_patch = d_model_A.output_shape[1]\n",
    "    trainA, trainB = dataset\n",
    "    poolA, poolB = list(), list()\n",
    "    bat_per_epo = int(len(trainA) / n_batch)\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "\n",
    "    for i in range(n_steps):\n",
    "\n",
    "        X_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch)\n",
    "        X_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch)\n",
    "\n",
    "        X_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\n",
    "        X_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\n",
    "\n",
    "        X_fakeA = update_image_pool(poolA, X_fakeA)\n",
    "        X_fakeB = update_image_pool(poolB, X_fakeB)\n",
    "\n",
    "        g_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n",
    "\n",
    "        dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n",
    "        dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n",
    "\n",
    "        g_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n",
    "\n",
    "        dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n",
    "        dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n",
    "\n",
    "        print('>%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))\n",
    "\n",
    "        if (i+1) % (bat_per_epo * 1) == 0:\n",
    "\n",
    "            summarize_performance(i, g_model_AtoB, trainA, 'AtoB')\n",
    "\n",
    "            summarize_performance(i, g_model_BtoA, trainB, 'BtoA')\n",
    "        if (i+1) % (bat_per_epo / 10) == 0:\n",
    "\n",
    "            save_models(i, g_model_AtoB, g_model_BtoA)\n",
    "\n",
    "\n",
    "dataset = load_real_samples('../data/genderswaptest.npz')\n",
    "print('Loaded', dataset[0].shape, dataset[1].shape)\n",
    "\n",
    "image_shape = dataset[0].shape[1:]\n",
    "\n",
    "g_model_AtoB = generator(image_shape)\n",
    "\n",
    "g_model_BtoA = generator(image_shape)\n",
    "\n",
    "d_model_A = discriminator(image_shape)\n",
    "\n",
    "d_model_B = discriminator(image_shape)\n",
    "\n",
    "c_model_AtoB = composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape)\n",
    "\n",
    "c_model_BtoA = composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape)\n",
    "\n",
    "train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First results \n",
    "\n",
    "We have trained these models on 3000 male and 3000 female photos, through 5 epochs. It run approximately 8 hours on Google Cloud Virtual Machine. For now, we think it is too less epochs and too small dataset for such a complicated structure of neural networks, if we want to see some usable transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](models/AtoB_generated_plot_015080.png)\n",
    "![](models/BtoA_generated_plot_015080.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Submission - Gender Swap\n",
    "## Adam Fábry, Dominik Feješ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "The motivation behind this project is that a lot of things surounding photos can be achieved with neural networks. One particular thing that can be easily achieved is a gender swap. The idea behind this project is that it might be fun to experiment with a neural network trying to change the appearence of people's faces, as well as seeing how would people look if they were the opposite gender.\n",
    "\n",
    "### Related work\n",
    "One very similar task was discussed in a paper: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks by Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros at Berkeley AI Research (BAIR) laboratory, UC Berkeley. The authors present way to translate source image into desired output without paired examples, which is very similar to the problem we are trying to solve, as in their paper they had source images that had to be translated to a similar image with different details (which in our case will be the gender swap).\n",
    "\n",
    "### Datasets\n",
    "The internet is filled with a lot of images and datasets consisting of male and female faces. Most datasets that distinguish the difference between male and female faces are the datasets that are used for gender classification. We are considering using the following datasets:\n",
    "\n",
    "CelebA: http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
    "Men/Women Classification Dataset: https://www.kaggle.com/playlist/men-women-classification\n",
    "IMDB-WIKI: https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/\n",
    "\n",
    "### High-level solution proposal\n",
    "We are not really sure how we are going to solve this project, but we have a general idea. The first one is using GANs (Generative neural networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "We have used CycleGAN for GenderSwap. The architecture is comprised of four models, two discriminator models, and two generator models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](../img/GanDiagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](../img/GanDiagram2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>discriminator</b> is a deep convolutional neural network that performs image classification. It predicts the likelihood of wheter the input image is real or fake image. We use 2 discriminator models, one for domainA - male photos, and one for domainB - female photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "\n",
    "def discriminator(image_shape):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    in_image = Input(shape=image_shape)\n",
    "    \n",
    "    layer = Conv2D(64, (4,4), strides=(2,2),padding='same',kernel_initializer=init)(in_image)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "\n",
    "    layer = Conv2D(128, (4,4), strides=(2,2),padding='same',kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "\n",
    "    layer = Conv2D(256, (4,4), strides=(2,2),padding='same',kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "\n",
    "    layer = Conv2D(512, (4,4), strides=(2,2),padding='same',kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "\n",
    "    layer = Conv2D(512, (4,4), padding='same',kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "\n",
    "    patch_out = Conv2D(1,(4,4), padding='same', kernel_initializer=init)(layer)\n",
    "\n",
    "    model = Model(in_image, patch_out)\n",
    "\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=0.0002,beta_1=0.5),loss_weights=[0.5])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>generator</b> takes care of generating the target image (for example generating a female photo from male photo). The generator will generate new fake images, that will be fed to descriminator mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(n_filters, input_layer):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "\n",
    "    layer = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    layer = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "\n",
    "    layer = Concatenate()([layer, input_layer])\n",
    "\n",
    "    return layer\n",
    "\n",
    "def generator(image_shape, n_resnet=9):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    in_image = Input(shape=image_shape)\n",
    "\n",
    "    layer = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    layer = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    layer = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    for _ in range(n_resnet):\n",
    "        layer = resnet_block(256,layer)\n",
    "    \n",
    "    layer = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    layer = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    layer = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    out_image = Activation('tanh')(layer)\n",
    "\n",
    "    model = Model(in_image, out_image)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator models are trained with the associated discriminator. They are trying to generate an image, that will predicted as real by the descriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_model(g_model_1,d_model,g_model_2,image_shape):\n",
    "    g_model_1.trainable = True\n",
    "    d_model.trainable = False\n",
    "    g_model_2.trainable = False\n",
    "\n",
    "    input_gen = Input(shape=image_shape)\n",
    "    gen1_out = g_model_1(input_gen)\n",
    "    output_d = d_model(gen1_out)\n",
    "\n",
    "    input_id = Input(shape=image_shape)\n",
    "    output_id = g_model_1(input_id)\n",
    "\n",
    "    output_f = g_model_2(gen1_out)\n",
    "\n",
    "    gen2_out = g_model_2(input_id)\n",
    "    output_b = g_model_1(gen2_out)\n",
    "\n",
    "    model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n",
    "\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "    model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "from numpy import ones\n",
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "from numpy.random import randint\n",
    "from matplotlib import pyplot\n",
    "from os import listdir\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    entriesA = listdir(path+'male/')\n",
    "    entriesB = listdir(path+'female/')\n",
    "    domainA = list()\n",
    "    domainB = list()\n",
    "    print('Loading male images')\n",
    "    lenA = len(entriesA)\n",
    "    lenB = len(entriesB)\n",
    "    for e in entriesA:\n",
    "        img = load_img(path+'male/'+e)\n",
    "        img = img_to_array(img)\n",
    "        domainA.append(img)\n",
    "    print('Loading female images')\n",
    "    for e in entriesB:\n",
    "        img = load_img(path+'female/'+e)\n",
    "        img = img_to_array(img)\n",
    "        domainB.append(img)\n",
    "    return asarray(domainA), asarray(domainB)\n",
    "\n",
    "def load_dataset_test(path):\n",
    "    entriesA = listdir(path+'testMale/')\n",
    "    entriesB = listdir(path+'testFemale/')\n",
    "    domainA = list()\n",
    "    domainB = list()\n",
    "    print('Loading male test images')\n",
    "    lenA = len(entriesA)\n",
    "    lenB = len(entriesB)\n",
    "    for e in entriesA:\n",
    "        img = load_img(path+'testMale/'+e)\n",
    "        img = img_to_array(img)\n",
    "        domainA.append(img)\n",
    "    print('Loading female test images')\n",
    "    for e in entriesB:\n",
    "        img = load_img(path+'testFemale/'+e)\n",
    "        img = img_to_array(img)\n",
    "        domainB.append(img)\n",
    "    return asarray(domainA), asarray(domainB)\n",
    "\n",
    "def generate_real_samples(dataset,n_samples, patch_shape):\n",
    "    ix = randint(0, dataset.shape[0],n_samples)\n",
    "    X = dataset[ix]\n",
    "    Y = ones((n_samples,patch_shape,patch_shape,1))\n",
    "    return X, Y\n",
    "\n",
    "def generate_fake_samples(g_model, dataset, patch_shape):\n",
    "    X = g_model.predict(dataset)\n",
    "    Y = zeros((len(X),patch_shape,patch_shape,1))\n",
    "    return X, Y\n",
    "\n",
    "def save_models(epoch, g_model_AtoB, g_model_BtoA):\n",
    "    path = '../../models/'\n",
    "    AtoB = 'g_model_AtoB_%03d.h5' % (epoch+1)\n",
    "    BtoA = 'g_model_BtoA_%03d.h5' % (epoch+1)\n",
    "    g_model_AtoB.save(path+AtoB)\n",
    "    g_model_BtoA.save(path+BtoA)\n",
    "\n",
    "def summarize_performance(epoch, g_model, trainX, name, n_samples=5):\n",
    "    path = '../../performance/'\n",
    "    X_in, _ = generate_real_samples(trainX, n_samples, 0)\n",
    "    X_out, _ = generate_fake_samples(g_model, X_in, 0)\n",
    "    X_in = (X_in + 1) / 2.0\n",
    "    X_out = (X_out + 1) / 2.0\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(2, n_samples, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_in[i])\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(2, n_samples, 1 + n_samples + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_out[i])\n",
    "    filename1 = '%s_generated_plot_%03d.png' % (name, (epoch+1))\n",
    "    pyplot.savefig(path+filename1)\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_image_pool(pool, images, max_size=50):\n",
    "    selected = list()\n",
    "    for image in images:\n",
    "        if len(pool) < max_size:\n",
    "            pool.append(image)\n",
    "            selected.append(image)\n",
    "        elif random() < 0.5:\n",
    "            selected.append(image)\n",
    "        else:\n",
    "            ix = randint(0, len(pool))\n",
    "            selected.append(pool[ix])\n",
    "            pool[ix] = image\n",
    "    return asarray(selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train() function takes all six models (two discriminators, two generators, and two composite models) as arguments along with the dataset and trains the models.</p>\n",
    "The train() funciton uses the step() function (where the training itself is done) on all six models for a number of times (number of epochs * number of steps in an epoch), and after each epoch, performance of generators is summarized as an .png file (5 examples of MtoF transformation and vice versa) alongside a saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, domainA, domainB):\n",
    "\tn_epochs, n_batch, = 35, 1\n",
    "\tn_patch = d_model_A.output_shape[1]\n",
    "\t#trainA, trainB = dataset\n",
    "\tpoolA, poolB = list(), list()\n",
    "\tn_steps = int(len(domainA) / n_batch)\n",
    "\n",
    "\tfor i in range(n_epochs):\n",
    "\t\tprint('Epoch:',i)\n",
    "\t\tfor j in range(n_steps):\n",
    "\t\t\tX_realA, y_realA = generate_real_samples(domainA, n_batch, n_patch)\n",
    "\t\t\tX_realB, y_realB = generate_real_samples(domainB, n_batch, n_patch)\n",
    "\n",
    "\t\t\tX_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\n",
    "\t\t\tX_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\n",
    "\n",
    "\t\t\tX_fakeA = update_image_pool(poolA, X_fakeA)\n",
    "\t\t\tX_fakeB = update_image_pool(poolB, X_fakeB)\n",
    "\n",
    "\t\t\tg_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n",
    "\n",
    "\t\t\tdA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n",
    "\t\t\tdA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n",
    "\n",
    "\t\t\tg_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n",
    "\n",
    "\t\t\tdB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n",
    "\t\t\tdB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n",
    "\t\t\tprint('Step: ',j+1, '\\ndA[',dA_loss1,dA_loss2,']\\ndB[',dB_loss1,dB_loss2,']\\ng[',g_loss1,g_loss2,']\\n-------------------------')\n",
    "\n",
    "\t\tsummarize_performance(i, g_model_AtoB, domainA, 'AtoB')\n",
    "\t\tsummarize_performance(i, g_model_BtoA, domainB, 'BtoA')  \n",
    "\t\tsave_models(i, g_model_AtoB, g_model_BtoA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "We can test the saved models with this code. We only need an input image of size 256x256. The syntax is: python3 generate.py <filename_of_AtoB_model> <filenamename_of_BtoA_model> <filename_of_picture> <m/f>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.models import load_model\n",
    "from numpy import vstack, expand_dims\n",
    "from loadingdata import load_dataset\n",
    "from matplotlib import pyplot\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "\n",
    "\n",
    "def image_to_translate(path):\n",
    "    img = load_img(path)\n",
    "    img = img_to_array(img)\n",
    "    img = expand_dims(img, 0)\n",
    "    img = (img - 127.5) / 127.5\n",
    "    return img\n",
    "\n",
    "def translate(domain, img, AtoB, BtoA):\n",
    "    if domain == 'm':\n",
    "        generated = AtoB.predict(img)\n",
    "        reconstructed = BtoA.predict(generated)\n",
    "    else:\n",
    "        generated = BtoA.predict(img)\n",
    "        reconstructed = AtoB.predict(generated)\n",
    "    return generated, reconstructed\n",
    "\n",
    "def save_img(img, generated, reconstructed,path):\n",
    "    images = vstack((img,generated,reconstructed))\n",
    "    titles = ['Input','Generated','Reconstructed']\n",
    "    images = (images + 1) / 2.0\n",
    "    for i in range(len(images)):    \n",
    "        pyplot.subplot(1,len(images),i+1)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(images[i])\n",
    "        pyplot.title(titles[i])\n",
    "    pyplot.savefig(path+'translation.png')\n",
    "\n",
    "# Generate translation with: python3 generate.py [name of AtoB model] [name of BtoA model] [picture to translate] [m/f]\n",
    "# Models should be placed in models folder in the repo and image to translate should be placed in translate folder in repo.\n",
    "\n",
    "path_models = '../../models/'\n",
    "path_image = '../../translate/'\n",
    "\n",
    "cust = {'InstanceNormalization': InstanceNormalization}\n",
    "AtoB = load_model(path_models+sys.argv[1],cust)\n",
    "BtoA = load_model(path_models+sys.argv[2],cust)\n",
    "\n",
    "img = image_to_translate(path_image+sys.argv[3])\n",
    "\n",
    "generated, reconstructed = translate(sys.argv[4],img,AtoB,BtoA)\n",
    "save_img(img,generated,reconstructed,path_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "We tried training this model with the whole dataset, but it did not work, and we also tried lowering the number of pictures to take. The first acceptable number was around 3500 pictures for each domain. We chose to keep the number of pictures to 3000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training was supposed to take 35 epochs, with batchsize 1 (more than that did not work, and the memory was not big enough). However, one epoch took around 2,5 hours so in the end we managed to train the model for 18 epochs. We have generator models saved from each epoch as well as the performance of the generator model, but the generator models however were too big to even push to repo.\n",
    "\n",
    "To see a quick sneak peak of how well the generator was trained (as we cannot determine it any other way reliably) we chose to have 5 pictures translated from each domain at the end of each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Results after first epoch:\n",
    "![generated_plot](../performance/AtoB_generated_plot_001.png)\n",
    "![generated_plot](../performance/BtoA_generated_plot_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results after 5th epoch:\n",
    "![generated_plot](../performance/AtoB_generated_plot_005.png)\n",
    "![generated_plot](../performance/BtoA_generated_plot_005.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the first features of genderswap developping (applying make-up to male faces, and removing make-up from female faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results after 10th epoch:\n",
    "![generated_plot](../performance/AtoB_generated_plot_010.png)\n",
    "![generated_plot](../performance/BtoA_generated_plot_010.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were satisfied with the results after this epoch, because we can also see the network trying to create artificial facial hair on female faces (first easily recognizable transformation on female faces)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results after 15th epoch:\n",
    "![generated_plot](../performance/AtoB_generated_plot_015.png)\n",
    "![generated_plot](../performance/BtoA_generated_plot_015.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the men are still being transformed with makeup application. For female, we can see that the application of facial hair decreased, but the network still applied it on some faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results after 18th and the last epoch:\n",
    "![generated_plot](../performance/AtoB_generated_plot_018.png)\n",
    "![generated_plot](../performance/BtoA_generated_plot_018.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this epoch show that not only the network tries to remove the facial hair from males and apply make-up at the same time. However, the facial hair is only removed around the mouth. For female images it is the complete opposite: removing make-up and adding facial hair (surprisingly). We can see a better application of facial hair in these examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The results are better than we expected on such a small dataset. To get the best results, we would need to use the whole dataset or at least a half of it, however it would take much longer to train even one epoch. The training took a long time so we did not even have time to experiment.\n",
    "\n",
    "We did not even manage to save logs, as we were struggling with getting it to work at all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

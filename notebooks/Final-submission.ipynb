{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "## Architecture\n",
    "We have used CycleGAN for GenderSwap. The architecture is comprised of four models, two discriminator models, and two generator models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](img/GanDiagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](img/GanDiagram2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>discriminator</b> is a deep convolutional neural network that performs image classification. It predicts the likelihood of wheter the input image is real or fake image. We use 2 discriminator models, one for domainA - male photos, and one for domainB - female photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "\n",
    "def discriminator(image_shape):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    in_image = Input(shape=image_shape)\n",
    "    \n",
    "    layer = Conv2D(64, (4,4), strides=(2,2),padding='same',kernel_initializer=init)(in_image)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "\n",
    "    layer = Conv2D(128, (4,4), strides=(2,2),padding='same',kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "\n",
    "    layer = Conv2D(256, (4,4), strides=(2,2),padding='same',kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "\n",
    "    layer = Conv2D(512, (4,4), strides=(2,2),padding='same',kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "\n",
    "    layer = Conv2D(512, (4,4), padding='same',kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = LeakyReLU(alpha=0.2)(layer)\n",
    "\n",
    "    patch_out = Conv2D(1,(4,4), padding='same', kernel_initializer=init)(layer)\n",
    "\n",
    "    model = Model(in_image, patch_out)\n",
    "\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=0.0002,beta_1=0.5),loss_weights=[0.5])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>generator</b> takes care of generating the target image (for example generating a female photo from male photo). The generator will generate new fake images, that will be fed to descriminator mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(n_filters, input_layer):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "\n",
    "    layer = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    layer = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "\n",
    "    layer = Concatenate()([layer, input_layer])\n",
    "\n",
    "    return layer\n",
    "\n",
    "def generator(image_shape, n_resnet=9):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    in_image = Input(shape=image_shape)\n",
    "\n",
    "    layer = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    layer = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    layer = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    for _ in range(n_resnet):\n",
    "        layer = resnet_block(256,layer)\n",
    "    \n",
    "    layer = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    layer = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "\n",
    "    layer = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(layer)\n",
    "    layer = InstanceNormalization(axis=-1)(layer)\n",
    "    out_image = Activation('tanh')(layer)\n",
    "\n",
    "    model = Model(in_image, out_image)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator models are trained with the associated discriminator. They are trying to generate an image, that will predicted as real by the descriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_model(g_model_1,d_model,g_model_2,image_shape):\n",
    "    g_model_1.trainable = True\n",
    "    d_model.trainable = False\n",
    "    g_model_2.trainable = False\n",
    "\n",
    "    input_gen = Input(shape=image_shape)\n",
    "    gen1_out = g_model_1(input_gen)\n",
    "    output_d = d_model(gen1_out)\n",
    "\n",
    "    input_id = Input(shape=image_shape)\n",
    "    output_id = g_model_1(input_id)\n",
    "\n",
    "    output_f = g_model_2(gen1_out)\n",
    "\n",
    "    gen2_out = g_model_2(input_id)\n",
    "    output_b = g_model_1(gen2_out)\n",
    "\n",
    "    model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n",
    "\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "    model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "from numpy import ones\n",
    "from numpy import zeros\n",
    "from numpy.random import randint\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def load_real_samples(filename):\n",
    "    data = load(filename)\n",
    "    X1, X2 = data['arr_0'], data['arr_1']\n",
    "\n",
    "    X1 = (X1 - 127.5) / 127.5\n",
    "    X2 = (X2 - 127.5) / 127.5\n",
    "    return [X1,X2]\n",
    "\n",
    "def generate_real_samples(dataset,n_samples, patch_shape):\n",
    "    ix = randint(0, dataset.shape[0],n_samples)\n",
    "    X = dataset[ix]\n",
    "    Y = ones((n_samples,patch_shape,patch_shape,1))\n",
    "    return X, Y\n",
    "\n",
    "def generate_fake_samples(g_model, dataset, patch_shape):\n",
    "    X = g_model.predict(dataset)\n",
    "    Y = zeros((len(X),patch_shape,patch_shape,1))\n",
    "    return X, Y\n",
    "\n",
    "# This function will save each generator model to the current directory in H5 format,\n",
    "# including the training iteration number in the filename\n",
    "def save_models(step, g_model_AtoB, g_model_BtoA):\n",
    "    path = '../../models/'\n",
    "    filename1 = 'g_model_AtoB_%06d.h5' % (step+1)\n",
    "    g_model_AtoB.save(path+filename1)\n",
    "    filename2 = 'g_model_BtoA_%06d.h5' % (step+1)\n",
    "    g_model_BtoA.save(path+filename2)\n",
    "\n",
    "# This function uses a given generator model to generate translated versions of\n",
    "# a few randomly selected source photographs and saves the plot to file.\n",
    "def summarize_performance(step, g_model, trainX, name, n_samples=5):\n",
    "    path = '../../models/'\n",
    "    X_in, _ = generate_real_samples(trainX, n_samples, 0)\n",
    "    X_out, _ = generate_fake_samples(g_model, X_in, 0)\n",
    "    X_in = (X_in + 1) / 2.0\n",
    "    X_out = (X_out + 1) / 2.0\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(2, n_samples, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_in[i])\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(2, n_samples, 1 + n_samples + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X_out[i])\n",
    "    filename1 = '%s_generated_plot_%06d.png' % (name, (step+1))\n",
    "    pyplot.savefig(path+filename1)\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_image_pool(pool, images, max_size=50):\n",
    "    selected = list()\n",
    "    for image in images:\n",
    "        if len(pool) < max_size:\n",
    "            pool.append(image)\n",
    "            selected.append(image)\n",
    "        elif random() < 0.5:\n",
    "            selected.append(image)\n",
    "        else:\n",
    "            ix = randint(0, len(pool))\n",
    "            selected.append(pool[ix])\n",
    "            pool[ix] = image\n",
    "    return asarray(selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train() function takes all six models (two discriminators, two generators, and two composite models) as arguments along with the dataset and trains the models.</p>\n",
    "The train() funciton uses the step() function (where the training itself is done) on all six models for a number of times (number of epochs * number of steps in an epoch), and after each epoch, performance of generators is summarized as an .png file (5 examples of MtoF transformation and vice versa) alongside a saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset):\n",
    "\tn_epochs, n_batch, = 1, 10\n",
    "\tn_patch = d_model_A.output_shape[1]\n",
    "\ttrainA, trainB = dataset\n",
    "\tpoolA, poolB = list(), list()\n",
    "\tn_steps = int(len(trainA) / n_batch)\n",
    "\n",
    "\tfor i in range(n_epochs):\n",
    "\t\tprint('Epoch:',i)\n",
    "\t\tfor j in range(n_steps):\n",
    "\t\t\tstep(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, trainA, trainB, n_batch, j, poolA, poolB, n_patch)\n",
    "        summarize_performance(i, g_model_AtoB, trainA, 'AtoB')\n",
    "\t\tsummarize_performance(i, g_model_BtoA, trainB, 'BtoA')  \n",
    "\t\tsave_models(i, g_model_AtoB, g_model_BtoA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step() function first takes a number of real samples for A and B domain (A stands for male and B for female), followed by generating a number of fake samples (generating with corresponding generators) from the real pictures, chosen in the previous step. Next step consists of updating corresponding image pools with fake images. Then the training of composite model comes into play, followed by training discriminator. This happens two times, for each composite model (Female->Male->Female and Male->Female->Male)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, trainA, trainB, n_batch, i, poolA, poolB, n_patch):\n",
    "\tX_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch)\n",
    "\tX_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch)\n",
    "\n",
    "\tX_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\n",
    "\tX_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\n",
    "\n",
    "\tX_fakeA = update_image_pool(poolA, X_fakeA)\n",
    "\tX_fakeB = update_image_pool(poolB, X_fakeB)\n",
    "\n",
    "\tg_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n",
    "\n",
    "\tdA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n",
    "\tdA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n",
    "\n",
    "\tg_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n",
    "\n",
    "\tdB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n",
    "\tdB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n",
    "\tprint('Step: ',i+1, '\\ndA[',dA_loss1,dA_loss2,']\\ndB[',dB_loss1,dB_loss2,']\\ng[',g_loss1,g_loss2,']\\n-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First results \n",
    "\n",
    "We have trained these models on 3000 male and 3000 female photos, through 5 epochs. It ran for approximately 8 hours on Google Cloud Virtual Machine. We think the dataset and epoch number is too small for any interesting and noteworthy results, as the this neural network is complicated. However, a small instances of slight differences can be seen on pictures, which means that the neural network tries to do what it is supposed to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](models/AtoB_generated_plot_015080.png)\n",
    "![](models/BtoA_generated_plot_015080.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
